{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader   \n",
    " \n",
    "import pyspch.sp as Sps\n",
    "import pyspch.display as Spd\n",
    "import pyspch.core as Spch\n",
    "\n",
    "from nn import utils\n",
    "from nn import corpus\n",
    "from nn import datasets\n",
    "from nn import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "timit_path = 'W:/timit/CDdata/timit/' # to extract corpus, features, labels\n",
    "root_path = 'D:/gitlab/psi/compi1234/nn/' # to read/write corpus, features, labels\n",
    "os.chdir(root_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TIMIT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIMIT - read corpus from disk\n",
    "timit_train = utils.read_txt('data/timit_train.corpus')\n",
    "timit_test = utils.read_txt('data/timit_test.corpus')\n",
    "timit_dummy = utils.read_txt('data/timit_dummy.corpus')\n",
    "\n",
    "# TIMIT - read meta from disk\n",
    "timit_meta = pd.read_csv('data/timit.meta', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### read MFCC39 features from disk\n",
    "read_feature_path = 'data/dummy/mfcc39'\n",
    "\n",
    "# initialize reader\n",
    "reader = corpus.ArrayReader(mode='numpy', extension='.npy')\n",
    "\n",
    "# TIMIT - read features\n",
    "spchdata_dummy = corpus.SpchData(timit_dummy)\n",
    "spchdata_dummy.read_features(read_feature_path, reader)\n",
    "\n",
    "# read feature_args from json\n",
    "feature_args_fname = os.path.join(read_feature_path, 'feature_args.json')\n",
    "feature_args = utils.read_json(feature_args_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract phone labels from segmentation \n",
    "seg_path = 'W:/timit/CDdata/timit/'\n",
    "seg_extension = \".phn\"\n",
    "\n",
    "# SpchData - extract labels\n",
    "spchdata_dummy.extract_labels(seg_path, feature_args, seg_extension)\n",
    "\n",
    "# Add padding\n",
    "lengths = spchdata_dummy.get_length_features()\n",
    "spchdata_dummy.pad_labels(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Torch DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMIT61 = ['aa', 'ae', 'ah', 'ao', 'aw', 'ax', 'ax-h', 'axr', 'ay', 'b', 'bcl', 'd', 'dcl', 'dh', 'dx', \n",
    "           'eh', 'el', 'en', 'epi', 'er', 'ey', 'f', 'g', 'gcl', 'h#', 'hh', 'hv', 'ih', 'ix', 'iy', 'jh',\n",
    "           'k', 'kcl', 'l', 'm', 'n', 'ng', 'nx', 'ow', 'p', 'pau', 'pcl', 'q', 'r', 's', 't', 'tcl', 'th', \n",
    "           'uw', 'ux', 'v', 'w', 'y', 'z']\n",
    "\n",
    "TIMIT41 = ['aa','ae', 'ah','ao','aw','er','ay','b','ch','d','dh','eh',\n",
    "           'm','ng','ey','f','g','hh','ih','iy','jh','k','l','n','ow',\n",
    "           'oy','p','r','s','sh','t','th','uh','uw','v','w','y','z','zh','sil','cl']\n",
    "\n",
    "\n",
    "timit61_41={ \n",
    "    'axr': 'er',\n",
    "    'em': 'm',\n",
    "    'eng': 'ng',\n",
    "    'nx': 'n',    \n",
    "    'hv': 'hh',\n",
    "    'kcl': 'cl',  'pcl': 'cl',  'tcl': 'cl',\n",
    "    'h#': 'sil', 'pau': 'sil' ,   'q': 'sil', \n",
    "    ## different from 48 mapping\n",
    "    'bcl': 'cl', 'dcl': 'cl',  'gcl': 'cl',\n",
    "    'epi': 'sil',\n",
    "    'dx': 't',\n",
    "    'ax-h': 'ah', 'ix': 'ih','ax': 'ah', 'ux': 'uh',\n",
    "    'el': 'l', 'en':'n' \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSet arguments\n",
    "splice_args = {'N': 5, 'stride': 2}\n",
    "\n",
    "# TIMIT label mapping\n",
    "lab41_2idx = {k: v for v, k in enumerate(TIMIT41)}\n",
    "lab2lab = {k: k for k in TIMIT61}\n",
    "lab2lab.update(timit61_41)\n",
    "lab2idx = {k: lab41_2idx[v] for k, v in lab2lab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SpchDataset\n",
    "fnames = spchdata_dummy.corpus\n",
    "features = spchdata_dummy.features\n",
    "labels = spchdata_dummy.labels\n",
    "\n",
    "# initialize\n",
    "spch_ds = datasets.SpchDataset(fnames, features, labels)\n",
    "\n",
    "# target encoding \n",
    "spch_ds.encode_target(lab2idx)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "# format\n",
    "spch_ds.to_tensor()\n",
    "spch_ds.to_device(device)\n",
    "\n",
    "# sampler (splicing during __getitem__)\n",
    "lengths = spchdata_dummy.get_length_features()\n",
    "spch_ds.set_sampler(lengths, splice_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FFDNN(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=702, out_features=1024, bias=True)\n",
       "    (1): Sigmoid()\n",
       "    (2): Linear(in_features=1024, out_features=768, bias=True)\n",
       "    (3): Sigmoid()\n",
       "    (4): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (5): Sigmoid()\n",
       "    (6): Linear(in_features=512, out_features=56, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model arguments (from dataset)\n",
    "in_dim = spch_ds.__getitem__(0)[0].shape[0]\n",
    "out_dim = len(spch_ds.lab2idx)\n",
    "hidden_layer_sizes = [1024, 768, 512]\n",
    "\n",
    "# model arguments\n",
    "model_args = {\n",
    "    'type': 'dnn',\n",
    "    'in_dim': in_dim,\n",
    "    'out_dim': out_dim,\n",
    "    'hidden_layer_sizes': hidden_layer_sizes\n",
    "}\n",
    "\n",
    "# model\n",
    "model = models.FFDNN(in_dim, out_dim, hidden_layer_sizes)\n",
    "\n",
    "# device\n",
    "\n",
    "spch_ds.to_device(device)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "training_args = {\n",
    "    'n_epoch': 50,\n",
    "    'patience': 15,\n",
    "    'lrn_rate': 0.00001,\n",
    "    'weight_decay': 0,\n",
    "    'batch_size': 64,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0,\n",
    "    'val_frac': 0.1,\n",
    "}\n",
    "\n",
    "# criterion & optimizer\n",
    "criterion = nn.CrossEntropyLoss() # applies softmax()\n",
    "optimizer = torch.optim.Adam(model.parameters(), \n",
    "                             lr=training_args['lrn_rate'], \n",
    "                             weight_decay=training_args['weight_decay'])\n",
    "\n",
    "# validation set\n",
    "if training_args['val_frac'] is not None:\n",
    "    n_ex = len(spch_ds)\n",
    "    n_valid = int(n_ex * training_args['val_frac'])\n",
    "    n_train = n_ex - n_valid\n",
    "    valid_ds, train_ds = torch.utils.data.random_split(spch_ds, [n_valid, n_train])\n",
    "else:\n",
    "    train_ds = spch_ds\n",
    "    \n",
    "# iterator\n",
    "train_dl = DataLoader(train_ds, \n",
    "    batch_size=training_args['batch_size'],\n",
    "    shuffle=training_args['shuffle'], \n",
    "    num_workers=training_args['num_workers'])\n",
    "\n",
    "if training_args['val_frac'] is not None:\n",
    "    valid_dl = DataLoader(valid_ds, \n",
    "        batch_size=training_args['batch_size'],\n",
    "        shuffle=training_args['shuffle'], \n",
    "        num_workers=training_args['num_workers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -- av. train loss per mini-batch 3.93\n",
      "\t -- av. validation loss per mini-batch 3.79\n",
      "Epoch 5 -- av. train loss per mini-batch 3.24\n",
      "\t -- av. validation loss per mini-batch 3.28\n",
      "Epoch 10 -- av. train loss per mini-batch 3.17\n",
      "\t -- av. validation loss per mini-batch 3.20\n",
      "Epoch 15 -- av. train loss per mini-batch 3.14\n",
      "\t -- av. validation loss per mini-batch 3.20\n",
      "Epoch 20 -- av. train loss per mini-batch 3.13\n",
      "\t -- av. validation loss per mini-batch 3.18\n",
      "Epoch 25 -- av. train loss per mini-batch 3.12\n",
      "\t -- av. validation loss per mini-batch 3.17\n",
      "Epoch 30 -- av. train loss per mini-batch 3.11\n",
      "\t -- av. validation loss per mini-batch 3.18\n",
      "Epoch 35 -- av. train loss per mini-batch 3.11\n",
      "\t -- av. validation loss per mini-batch 3.17\n",
      "Epoch 40 -- av. train loss per mini-batch 3.11\n",
      "\t -- av. validation loss per mini-batch 3.14\n",
      "Epoch 45 -- av. train loss per mini-batch 3.10\n",
      "\t -- av. validation loss per mini-batch 3.14\n"
     ]
    }
   ],
   "source": [
    "# arguments\n",
    "every = 5 \n",
    "current_epoch = 0\n",
    "\n",
    "# train model\n",
    "tr_loss, val_loss, epoch = models.train(model, train_dl, criterion, optimizer, clip=None, \n",
    "    current_epoch=current_epoch, n_epochs=training_args['n_epoch'],\n",
    "    valid_dl=valid_dl, patience=training_args['patience'], every=every)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "# int2label\n",
    "idx2lab= {v: k for k, v in spch_ds.lab2idx.items()}\n",
    "labels = list(idx2lab.values())\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate with Dataloader\n",
    "\n",
    "# evaluate \n",
    "loss = model.evaluate(train_dl, criterion)\n",
    "cm = model.evaluate_cm(train_dl)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07bbb230afa188f6fcf784790a512a027d4aaebd8b449a47936f42b92e92fbab"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
